MODELS
The Adadelta, no dropout, trained on GCN preprocessed data; adadelta, no dropout, trained on 
unpreprocessed data; adadelta, with dropout, trained on unpreprocessed data; and non-adadelta, 
no dropout, trained on unpreprocessed data models are located in /data/lisatmp2/keypoints_detector/ 
in joblib format (pickle and cPickle would both bug out while trying to read the files while joblib 
is fine with it).

DATA
-use:
The files named all_{which_set}.h5 in ${PYLEARN2_DATA_PATH}/faces/hdf5/ 
contain the non-preprocessed version of the AFLW, LFPW, ArFaceDB, and Multi-Pie data,
where each image has been first converted to RGB and then cropped around the face based 
on available bounding box, or, failing that, the keypoints' maximum and minimum x and 
y (or, failing that, the image's boundaries), to which a 20% frame was added so the frame 
isn't too tight. The bound was then blown up from the smaller side toward the larger one, 
until either the sizes matched a square or the image's edges were encountered. If such
an edge was found, the other side of the image was reduced to match the blown-up smaller
side. The keypoints still inside the bounds were kept, while the others were discarded. 
Finally, the images were resized to 96x96.

The preprocessed data is the same data, to which Global Contrast Normalization (image-wise 
(X-mean)/std. dev.) preprocessing was applied. The files containing this data are called GCN_{which_set}.h5 
and live in the same folder as the non-preprocessed data.

The perturbed data is a perturbed version of the original data without preprocessing. The
test and valid set contain a subset of the unperturbed data (together they make the full set),
while the train set contains only perturbed data (a different perturbation for each image, 4 times
the original set in the same order).
They live in the same folder as the other sets and are named perturbed_{which_set}.h5.

-generate:
Supposing the data is already available as .npy, then run the following scripts, in order:
* comboDS (after having set the appropriate paths in the file). This will generate .h5 
files from the .npy. BEWARE that the original .npy were incorrectly formated, so the 
make_data function in this script will automatically correct the problem before dumping 
the data to .h5 and you should make sure to remove this behavior if it is no longer relevant 
to you.
* [optional] gen_perms. This will create a new .h5 file with the original data
as well as perturbations thereof.
* [optional] split_h5 (with arguments: .h5 complete filename, which_set, start and stop). 
This will split the .h5 file into, for example, train, test and validation sets (at your discretion).
* make_h5_targets. This will turn the keypoints target into density vectors describing a gaussian 
distribution around their x (first vector of a pair) and y (second vector of a pair). The 
result will be a (batch_size, 2*num_keypoints, num_valid_{x,y}_coords) matrix.

If the data is not already available as .npy, follow the instructions in 
${PYLEARN2_DATA_PATH}/faces/hdf5/ for generating the data.

MODEL
The model is based on Nicholas Leonard's model, which was the best model entered in the kaggle 
competition. It is composed of 2 convrect layers on top of which is a maxout layer, followed by 
a multisoftmax layer (a softmax that applies in different dimensions separately, so as to output 
a separate vector for the x and y coordinates). This model was tested in the kaggle competition 
with the kaggle data and won first place, outclassing Nicholas' original model. The cost function 
is MLPCost, which is a simple least-square error cost. It was modified from an error sum to an error 
mean, but this change should be reverted (see below). The training algorithm is KeypointsSGD, which 
is simply an SGD adapted to work with this model. Some changes were made to pylearn2 and Nicholas' 
files in order to make everything compatible with the new interface, which Nicholas' code didn't care 
about (but which DenseDesignMatrixPytable, which ComboDatasetPyTable, the main dataset class, cares about), 
and to make it more compatible with stream-based or lazy-loaded data. The branch with which the model 
works is available at:
git@github.com:git-/pylearn2
The rest of the required files are available in this directory.
As mentioned, the dataset object is ComboDatasetPyTable, available from comboDS.py.

NOTE: since the model uses CudaConvnet for convolution, it cannot run on the CPU. Furthermore, without
Adadelta or dropout, with a batch_size of 64, and the default image size, it takes about 5.5GB
of VRAM to run the model (which means only titans can handle it).
Adadelta without dropout and a batch_size of 48 can take the whole 6GB, so make sure the GPU has
freed all its memory before starting the experiment (lacking as little as 120MB is NOT OK).

ALREADY TRIED
* Without ADADELTA
- 0.1
Cost: sum or mean, with dropout, epoch 187
Overall bad results, only a few good detections with argmax, none with keypoint position expectation.

Cost: sum, no dropout, epoch 120
Better but still unsatisfying results. No apparent overfitting.

-0.025
Cost: sum, with dropout, epoch 130
Same as with LR 0.1

Cost: sum, no dropout, epoch 80
Better results still than no dropout with LR of 0.1, at around 1/3 heads correctly trained for and 1/4 of those
with accurate enough keypoints.

-0.0025
Cost: sum, no dropout, epoch 198
Worse results than with LR 0.025, better than with LR 0.1

-0.005
Cost: sum, no dropout, epoch 66
So far, somewhat better than LR 0.0025 at this point. Still worse than LR 0.025 at the same point.

* With ADADELTA
Cost: sum, no dropout, epoch 100
Best results so far by quite a bit. Still not good enough. Also, starts overfitting around epoch 50.

Cost: sum, with dropout, epoch 79
Should continue training for about 40 more epochs to see what happens but otherwise it converges and doesn't overfit,
but it's (as expected) much slower than without dropout.

Cost: sum, no dropout, epoch 87, GCN preprocessing
After 87 epochs, still not as good as the non-preprocessed data at ~50 epochs. Pretty close, and hasn't
overfit. However, the initial score was lower than without preprocessing (a bug?).
Nonetheless, the results are outstanding compared to anything else.
It is the only model with low enough variance in the output gaussians that points can be displayed via expectation instead of argmax.
While it's not quite there yet (many mistakes on obscured or very zoomed-in faces), I count around 1 in 5 image with actually bad keypoints where a face is really present.

Cost: sum, with dropout, epoch -, perturbed data
Didn't finish even one epoch after 22 hours, even though the perturbed set was no bigger than 4x the original train set (valid set being <2x as big).
NOTE: sending SIGTERM results in "Error freeing device pointer <addr> (unspecified launch failure)". It is possible that the error didn't properly stop execution
and instead locked up the process for the 22 hours.

TOOLS
testModel.py will display images with the model's predicted keypoints using either argmax or expectation, 
based on the provided path and set (changes have to be made in the script). It requires THEANO_FLAGS to be
set appropriately.
gen_perms.py takes an h5 file without gaussian targets and adds the original data as well as 
perturbations into a new h5 file.

TODO
* Visualize hidden layer weights to more accurately determine why the model isn't training as expected.
* Try mean cost again
* Try to get the perturbed data and model to work
* Try dataset-wise contrast normalization instead of GCN
